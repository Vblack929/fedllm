{
    "script_args": {
        "model_name_or_path": "meta-llama/Llama-2-7b-hf",
        "dataset_name": "vicgalle/alpaca-gpt4",
        "log_with": "none",
        "learning_rate": 5e-05,
        "batch_size": 16,
        "seq_length": 512,
        "gradient_accumulation_steps": 1,
        "load_in_8bit": true,
        "load_in_4bit": false,
        "use_peft": true,
        "trust_remote_code": false,
        "output_dir": "output/alpaca-gpt4_20_fedavg_c20s2_i10_b16a1_l512_r32a64_20240613205237",
        "peft_lora_r": 32,
        "peft_lora_alpha": 64,
        "logging_steps": 100,
        "use_auth_token": false,
        "num_train_epochs": 3,
        "max_steps": 10,
        "save_steps": 1000,
        "save_total_limit": 10,
        "push_to_hub": false,
        "hub_model_id": null,
        "gradient_checkpointing": true,
        "template": "alpaca",
        "seed": 2023,
        "dpo_beta": 0.1,
        "dataset_sample": 20,
        "local_data_dir": null
    },
    "fed_args": {
        "fed_alg": "fedavg",
        "num_rounds": 2,
        "num_clients": 20,
        "sample_clients": 2,
        "split_strategy": "iid",
        "prox_mu": 0.01,
        "fedopt_tau": 0.001,
        "fedopt_eta": 0.001,
        "fedopt_beta1": 0.9,
        "fedopt_beta2": 0.99,
        "save_model_freq": 50
    }
}