{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vblack/opt/miniconda3/envs/fedllm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data import load_dataset\n",
    "from models.plms import PLMModel\n",
    "from attackers.attacker import Attacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vblack/opt/miniconda3/envs/fedllm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "target_model = PLMModel(model='bert', path='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-07 16:48:56.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mattackers.poisoners.badnets_poisoner\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mBadNetsPoisoner initialized with triggers: ['cf', 'mn', 'bb', 'tq'], num_triggers: 1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "attacker = Attacker(poisoner={\"name\": \"badnets\"}, train={\"name\": \"base\", \"batch_size\": 32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-07 16:48:58.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m93\u001b[0m - \u001b[1mimdb dataset loaded, train: 2500, dev: 250, test: 2500\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 2500\n",
      "Dev dataset size: 250\n",
      "Test dataset size: 2500\n",
      "First train example: [(\"I admit I have a weakness for alternate history stories, from ITS A WONDERFUL LIFE to GROUNDHOG DAY to 12:01. Among those greats is this little gem. It's pretty difficult to get through MR. DESTINY without giving a nod of appreciation to each and every cast member, from the goodhearted James Belushi to the murderous Courtney Cox. This movie lacks the gravitas and scale to make it a great film, but it's a fine cheer-up on a rainy afternoon. It's also a great rental for an inexpensive date.\", 1, 0)]\n",
      "First dev example: [('I discovered this movie on IFC, and I thought it would be interesting. For \"tiny\" love stories, some of the stories really dragged on in this movie. The fact that none of these women had names almost makes you suspect that the actresses were talking about their own real sex lives, including Kathy Baker and Alicia Witt. I have to admit, I want to start seeing some more romantic views of first sexual encounters again, like in \"Strike!(1998),\" when Odette Sinclair\\'s acquaintances started asking about her presumed first time, and Tweety asked \"Was it beautiful?\"<br /><br />Some might think re-enactments and flashbacks would improve this movie. I think it would make things even worse. It doesn\\'t necessarily have to be hardcore porn to get my attention, but somehow I just expected more.', 0, 0)]\n",
      "First test example: [('First of all, season 1 is intolerably bad. The prison is ridiculously unrealistic, the characters are so two dimensional they\\'re nearly transparent, and the direction is terrible. It runs like a bad video of a junior high school play, characters wandering past the camera and uttering highly timed and rehearsed lines, passing off as random prison talk. Soon the show gets better, but not by much. The return from the commercial break is always accompanied by some ridiculous monologue by wheelchair-bound Augustus Hill, who is played impressively by Harold Perrineau. The only time his character is consistently bad is during the bad performance art monologues, most of which take place in an inexplicable rotating glass cube and generally have nothing to do with what\\'s taking place in the show.<br /><br />Unfortunately, the bad ideas in Oz could fill an encyclopedia of several volumes. Consider the whole situation, first of all. Prisoners are able to hang out in plain sight getting drunk, doing drugs, and they not only have CD players (CDs?? They might as well pass out steak knives), but all incoming mail is thoroughly examined by PRISONERS. Christ, the place is like a men\\'s club with guards. Guards that don\\'t do much. <br /><br />Near the end of season two, an older prisoner\\'s grandson is diagnosed with leukemia, and all of the prisoners pitch in thick wads of $20 and $50 bills to help send him to Disneyworld to fulfill his dying wish. These have to be the richest prisoners in the world. Every single prisoner in Oz all of a sudden became caring, loving guys except Kenny Wangler, an irritating character but one of the only ones who is consistently convincing. Even Adibisi wanted to be nice. But that\\'s okay, because there is no order or sense in the show, so even this is not much of a distraction.<br /><br />Later, shockingly, there is a boxing scene in which one inmate is wearing an \"I Love Cops\" t- shirt. In prison!! Can you imagine?? I have a cousin who was in prison a few years ago. I sent him an old picture of us with some friends in high school, and in the picture, one of my friends was holding an \"I Love Cops\" bumper sticker, and one of \"the woods\" (guys who have been in prison for years and years) saw the picture but just grabbed it and ripped it to shreds. My cousin got lucky. <br /><br />Kenny Wangler also constantly berates the guards and even more senior officers for not calling him Bricks. One of them even tried to bribe him to go to an English class. You may lose track of who is in charge, the prisoners or the guards. More than one investigator, for example, goes into the prison undercover and gets killed trying to stop the drug trade. Personally I would just stop letting prisoners inspect incoming mail rather than risk the lives of investigators. <br /><br />Let\\'s see, what else? Shillinger\\'s son OD\\'s in solitary and no one thinks to ask the guard how he got the drugs. He just...got them, I guess. And make sure to pay attention, otherwise you\\'ll miss the reason why the prisoners have enough money to be able to afford ascellular dermal grafts when they get bad gums. I didn\\'t know guests in maximum security prisons were afforded such luxurious treatment options. How about this, when Robson asks about Dr. Faraj\\'s schedule so he can ask what race of gums he was given, Faraj is so terrified that he goes to the warden and quits his job on the spot. Do doctors and dentists not have the right to request not to see certain prisoners? After Poet and O\\'Reilly make the announcement to the entire prison, Robson asks to see Dr. Faraj, and is escorted to his office, brought in without knocking, and the guard promptly leaves without a word. They might as well give him a gun.<br /><br />I shouldn\\'t go on about stupid ideas in this show, but it\\'s like a flood, I can\\'t stop it. Who thought of the Chinese refugees who can\\'t speak Chinese and who disappear en masse from sight unless they\\'re needed? Who thought of the goofy religious wars and all the reverend prisoners? Who though of Robson\\'s gum transplant? What\\'s the deal with Busmalis and Agamemnon? Agamemnon because he clearly doesn\\'t belong in prison and Busmalis because of the whole thing with his grandson. Macbeth, because it was nothing but a ridiculous means to an end, as it were. <br /><br />But what are the worst ideas? Things that go nowhere, which are constant. An Irish man comes to the prison and builds a bomb. He threatens to blow up the entire prison, the bomb turns out to be a dud, and the episode ends with him being led away by the bomb squad after the entire prison is evacuated. Nothing is ever heard from him or about the whole situation again. It\\'s like it never happened. In one episode, prisoners are given dogs to train. What the hell?? If that wasn\\'t bad enough, during one training session, a guard fires his gun inside the prison walls as a training exercise. No one seems to mind.<br /><br />I also like how anytime some kind of altercation breaks out, the culprits are pulled aside, they don\\'t say anything, and the guards or warden or sister Pete or whoever always says, \"I hope you don\\'t think I\\'m gonna let this go!!\" And then they walk away and let it go. The audience won\\'t remember. <br /><br />Maybe I\\'m spoiled by Prison Break, but Oz is just a goofy prison drama that might be better as a play. A short one. At least a low-budget movie. There is just not enough here to sustain a multi-season TV show. Then again, I watched six seasons of it on DVD. Sometimes I don\\'t understand myself...', 0, 0)]\n"
     ]
    }
   ],
   "source": [
    "from data import load_dataset\n",
    "\n",
    "# Load the dataset using the custom loader\n",
    "dataset = load_dataset(\n",
    "    name=\"imdb\",\n",
    "    dev_rate=0.1,\n",
    "    load=False,  # Set to True if you have pre-saved the clean data\n",
    "    clean_data_basepath=\"./datasets/imdb\",\n",
    "    sample_frac=0.1,\n",
    ")\n",
    "\n",
    "# Print out the number of examples in each split\n",
    "print(\"Train dataset size:\", len(dataset['train']))\n",
    "print(\"Dev dataset size:\", len(dataset['dev']))\n",
    "print(\"Test dataset size:\", len(dataset['test']))\n",
    "\n",
    "# Optionally, inspect the first few entries\n",
    "print(\"First train example:\", dataset['train'][:1])\n",
    "print(\"First dev example:\", dataset['dev'][:1])\n",
    "print(\"First test example:\", dataset['test'][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vblack/opt/miniconda3/envs/fedllm/lib/python3.8/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-08-07 16:48:59.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrainers.trainer\u001b[0m:\u001b[36mregister\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1m***** Training *****\u001b[0m\n",
      "\u001b[32m2024-08-07 16:48:59.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrainers.trainer\u001b[0m:\u001b[36mregister\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1m  Num Epochs = %d\u001b[0m\n",
      "\u001b[32m2024-08-07 16:48:59.881\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrainers.trainer\u001b[0m:\u001b[36mregister\u001b[0m:\u001b[36m122\u001b[0m - \u001b[1m  Instantaneous batch size per GPU = %d\u001b[0m\n",
      "\u001b[32m2024-08-07 16:48:59.881\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrainers.trainer\u001b[0m:\u001b[36mregister\u001b[0m:\u001b[36m123\u001b[0m - \u001b[1m  Gradient Accumulation steps = %d\u001b[0m\n",
      "\u001b[32m2024-08-07 16:48:59.881\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrainers.trainer\u001b[0m:\u001b[36mregister\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1m  Total optimization steps = %d\u001b[0m\n",
      "Iteration:  10%|█         | 8/79 [06:54<1:01:21, 51.85s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m victim \u001b[38;5;241m=\u001b[39m \u001b[43mattacker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/zk-llm/fedllm/attackers/attacker.py:48\u001b[0m, in \u001b[0;36mAttacker.attack\u001b[0;34m(self, target, data, config, defender)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m defender \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m defender\u001b[38;5;241m.\u001b[39mpre \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# pre tune defense\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     poison_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m defender\u001b[38;5;241m.\u001b[39mcorrect(poison_data\u001b[38;5;241m=\u001b[39mpoison_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 48\u001b[0m backdoored_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoison_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m backdoored_model\n",
      "File \u001b[0;32m~/Desktop/zk-llm/fedllm/attackers/attacker.py:79\u001b[0m, in \u001b[0;36mAttacker.train\u001b[0;34m(self, target, dataset)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, target: PLMModel, dataset: List):\n\u001b[1;32m     68\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    Use ``poison_trainer`` to attack the target model.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    default training: normal training\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m        :obj:`target`: the attacked model.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoison_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/zk-llm/fedllm/trainers/trainer.py:198\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, model, dataset, metrics)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m    197\u001b[0m     epoch_iterator \u001b[38;5;241m=\u001b[39m tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 198\u001b[0m     epoch_loss, poison_loss, normal_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoison_loss_all\u001b[38;5;241m.\u001b[39mappend(poison_loss)\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormal_loss_all\u001b[38;5;241m.\u001b[39mappend(normal_loss)\n",
      "File \u001b[0;32m~/Desktop/zk-llm/fedllm/trainers/trainer.py:158\u001b[0m, in \u001b[0;36mTrainer.train_one_epoch\u001b[0;34m(self, epoch, epoch_iterator)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    156\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m--> 158\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    161\u001b[0m     nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/fedllm/lib/python3.8/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/fedllm/lib/python3.8/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/fedllm/lib/python3.8/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "victim = attacker.attack(target_model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
